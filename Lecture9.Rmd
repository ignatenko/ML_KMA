---
title: "Lecture 8. Overview of methods"
author: "Олексій Ігнатенко"
date: "November 3, 2019"
output:
  ioslides_presentation:
    smaller: no
    widescreen: yes
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Дерева рішень

```{r}
library(party)
```

## Побудувати предиктор на основі регресії

```{r}
train <- readingSkills[c(1:120),]
test <- readingSkills[c(121:200),]

```

## Регресія

```{r}
lm_score <- lm(score ~ ., data = train)
show <- readingSkills
show$prediction <- predict(lm_score, show)
```


```{r}
ggplot(data = readingSkills, aes(age, score)) + 
  geom_point(aes(colour = factor(readingSkills$nativeSpeaker)))
```

## 

## Дерева рішень
 
> A Decision Tree is a Supervised Machine Learning algorithm which looks like an inverted tree, wherein each node represents a predictor variable (feature), the link between the nodes represents a Decision and each leaf node represents an outcome (response variable).  


##

```{r}
library(rpart)
# Create the tree.
  output.tree <- rpart(
  nativeSpeaker ~ age + shoeSize + score, 
  data = train)
```

##
```{r}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(output.tree)

```


## Випадкові ліси

```{r}
# Load the party package. It will automatically load other
# required packages.
library(randomForest)
```

##

```{r}

# Create the forest.
output.forest <- randomForest(nativeSpeaker ~ age + shoeSize + score, 
           data = readingSkills)
```

##

```{r}

# View the forest results.
print(output.forest) 

```


## 

```{r}
library(AppliedPredictiveModeling)
data(abalone)
```




## Базова модель

```{r}
library(ISLR)
data(Wage)
summary(Wage)
```

## Побудуємо лінійну залежність

```{r}
lm_wage <- lm(wage ~ age, data = Wage)
unseen <- data.frame(age = 60)
predict(lm_wage, unseen)
```

## Категорії проблем

- класифікація 
- регресія
- кластерізація


## Практичне застосування навчання з вчителем

```{r}
str(iris)
summary(iris)
```

## 

```{r}
set.seed(1)
library(rpart)
tree <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris, method = "class")
```

## 

```{r}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree)
```

## 

```{r}
unseen <- data.frame(Sepal.Length = c(5.3, 7.2),
                     Sepal.Width = c(2.9, 3.9),
                     Petal.Length = c(1.7, 5.4),
                     Petal.Width = c(0.8, 2.3))
```


##

```{r}
unseen
predict(tree, unseen, type = "class")
```


## Почитати про методи

https://bradleyboehmke.github.io/HOML/DT.html

## Набір Ames. Продаж будинків.

```{r}
ames <- read.csv('ames.csv', header = T)
str(ames)

```

## Семплінг

```{r}
# Using base R
set.seed(123)  # for reproducibility
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]

```


## Using caret package
```{r}
library(caret)

set.seed(123)  # for reproducibility

index_2 <- createDataPartition(ames$SalePrice, p = 0.7, list = FALSE)

train_2 <- ames[index_2, ]

test_2  <- ames[-index_2, ]

```

## Using rsample package
```{r}
library(rsample)
set.seed(123)  # for reproducibility

split_1  <- initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

```

##
```{r}
library(ggplot2)
ggplot(data = train_1, aes(SalePrice)) + 
  geom_density() + 
  geom_density(data = test_1,aes(SalePrice, colour = "red"))
```

## 

```{r}
model1 <- lm(SalePrice ~ Gr.Liv.Area, data = train_1)
summary(model1) 
```


## Метод KNN

K-nearest neighbor (KNN) is a very simple algorithm in which each observation is predicted based on its “similarity” to other observations. Unlike most methods in this book, KNN is a memory-based algorithm and cannot be summarized by a closed-form model. This means the training samples are required at run-time and predictions are made directly from the sample relationships.

## Як міряти відстань

```{r}
two_houses <- train_1[1:2, c("Gr.Liv.Area", "Year.Built")]
dist(two_houses, method = "euclidean")
dist(two_houses, method = "manhattan")
```

## Відстань між двома довільними будинками (Фічами)

```{r}
home1 <- ames[1,]
home2 <- ames[2,]

features <- c("Bedroom.AbvGr", "Year.Built")
dist(rbind(home1[,features], home2[,features]))
```

## Як працює КНН

```{r}
df <- data(iris) ##load data
 head(iris) ## see the studcture
```

##

```{r}
 ran <- sample(1:nrow(iris), 0.9 * nrow(iris)) 
 
 ##the normalization function is created
 nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
 ##Run nomalization on first 4 coulumns of dataset because they are the predictors
 iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], nor))
 
 summary(iris_norm)
```


##  

```{r}
iris_train <- iris_norm[ran,] 
##extract testing set
 iris_test <- iris_norm[-ran,] 
 ##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
 iris_target_category <- iris[ran,5]
 ##extract 5th column if test dataset to measure the accuracy
 iris_test_category <- iris[-ran,5]
```

##

```{r}
 library(class)
 ##run knn function
 pr <- knn(iris_train,iris_test,cl=iris_target_category,k=13)
 
 ##create confusion matrix
 tab <- table(pr,iris_test_category)
 
 accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
 accuracy(tab)
```

## Завдання. Лаба 3-4

https://github.com/ignatenko/reserve-price

1. Дані - торги прозорро. Три варанти - яйця, ...
2. Аналіз датасету і змінних.
3. Розділити на тренувальний і тестовий.
4. Побудувати регресійну модель (в залежності від характеру даних - лінійну або нелінійну). Оцінити точність прогнозу.
5. Побудувати дерево рішень для одної зі змінних (наприклад успішність, величина зменшення ціни при торгах). Побудувати модель випадковий ліс. Оцінити точність прогнозу.
6. Побудувати модель KNN. Оцінити точність прогнозу.

